{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/immangwu/COPO/blob/main/Copy_of_response_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqOkaEWymAI_"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "oJPEpdnUmKgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gspread\n",
        "import pandas as pd\n",
        "from google.colab import auth"
      ],
      "metadata": {
        "id": "hVR5Dh77me-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "#autenticating to google\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "9gbYu4SltPCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#defining my worksheet\n",
        "worksheet = gc.open('Survey data').sheet1\n",
        "#get_all_values gives a list of rows\n",
        "rows = worksheet.get_all_values()\n",
        "#Convert to a DataFrame\n",
        "df = pd.DataFrame(rows)\n",
        "df=df.iloc[:, 2]\n",
        "df"
      ],
      "metadata": {
        "id": "SMgqOxRfnhjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity\n"
      ],
      "metadata": {
        "id": "n5_8AjhiA2Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create preprocess_text function\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stop words\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    # Join the tokens back into a string\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "    return processed_text\n",
        "# apply the function df\n",
        "df = df.apply(preprocess_text)\n",
        "df"
      ],
      "metadata": {
        "id": "dPv6xf2tIMBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create get_sentiment function to handle multiple texts\n",
        "def get_sentiment(texts):\n",
        "    sentiments = []\n",
        "    for text in texts:\n",
        "        scores = analyzer.polarity_scores(text)\n",
        "        threshold = 0.1  # Adjust the threshold as needed\n",
        "        sentiment = 1 if scores['pos'] - scores['neg'] >= threshold else 0\n",
        "        sentiments.append(sentiment)\n",
        "    return sentiments\n"
      ],
      "metadata": {
        "id": "iC9ExJbetp8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize NLTK sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "# create get_sentiment function\n",
        "def get_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    sentiment = 1 if scores['pos'] > 0 else 0\n",
        "    return sentiment\n",
        "# apply get_sentiment function\n",
        "i=[]\n",
        "arr=[]\n",
        "i = df.apply(get_sentiment)\n",
        "arr = df.to_numpy()\n",
        "arr\n",
        "\n"
      ],
      "metadata": {
        "id": "YL7el2y7ts1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive=[];\n",
        "negative=[];\n",
        "for x in range(len(arr)):\n",
        "  if i[x]>0:\n",
        "    positive.append(arr[x]);\n",
        "  else:\n",
        "    negative.append(arr[x]);\n",
        "positiveStr = ' '.join(map(str, positive))\n",
        "negativeStr = ' '.join(map(str, negative))\n"
      ],
      "metadata": {
        "id": "fPriK6kAwc34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wordcloud\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display\n",
        "#import fileupload\n",
        "import io\n",
        "import sys\n",
        "\n",
        "\n",
        "def calculate_frequencies(file_contents,y):\n",
        "    # Here is a list of punctuations and uninteresting words you can use to process your text\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    uninteresting_words = [\"the\", \"a\", \"to\", \"if\", \"is\", \"it\", \"of\", \"and\", \"or\", \"an\", \"as\", \"i\", \"me\", \"my\", \\\n",
        "    \"we\", \"our\", \"ours\", \"you\", \"your\", \"yours\", \"he\", \"she\", \"him\", \"his\", \"her\", \"hers\", \"its\", \"they\", \"them\", \\\n",
        "    \"their\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \\\n",
        "    \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"but\", \"at\", \"by\", \"with\", \"from\", \"here\", \"when\", \"where\", \"how\", \\\n",
        "    \"all\", \"any\", \"both\",\"in\", \"each\", \"few\", \"more\", \"some\", \"such\", \"no\", \"nor\", \"too\", \"very\", \"can\", \"will\", \"just\",\"on\"]\n",
        "    a=file_contents.split()\n",
        "    new_list=[]\n",
        "    for i in a:\n",
        "        new=i.lower()\n",
        "        if new not in punctuations and new not in uninteresting_words:\n",
        "            new_list.append(new)\n",
        "    result = {}\n",
        "    for letter in new_list:\n",
        "        if letter.isalpha():\n",
        "            if letter not in result:\n",
        "                result[letter]=0\n",
        "            result[letter]+=1\n",
        "    #return result\n",
        "\n",
        "    # LEARNER CODE START HERE\n",
        "\n",
        "\n",
        "    #wordcloud\n",
        "    cloud = wordcloud.WordCloud(width=900,height=500, max_words=1628,relative_scaling=1,normalize_plurals=False )\n",
        "    cloud.generate_from_frequencies(result)\n",
        "    #storing\n",
        "    images_dir = '/content/drive/My Drive/Images'\n",
        "    if y==1:\n",
        "      cloud.to_file(f\"{images_dir}/Positive.jpg\");\n",
        "    else:\n",
        "      cloud.to_file(f\"{images_dir}/Negative.jpg\");\n",
        "\n",
        "\n",
        "    return cloud.to_array()\n"
      ],
      "metadata": {
        "id": "2uHqjhGu7Ui5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VhuxNiRDIkL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myimage = calculate_frequencies(positiveStr,1)\n",
        "myimage = calculate_frequencies(negativeStr,0)\n"
      ],
      "metadata": {
        "id": "N5bm3iI47Zt-"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}